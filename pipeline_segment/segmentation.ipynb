{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d517cd33",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import configparser\n",
    "import cv2\n",
    "import numpy as np\n",
    "from pero_ocr.core.layout import PageLayout\n",
    "from pero_ocr.document_ocr.page_parser import PageParser\n",
    "import re\n",
    "from ultralytics import YOLO\n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1f5192f",
   "metadata": {},
   "outputs": [
    {
     "ename": "error",
     "evalue": "OpenCV(4.11.0) D:\\a\\opencv-python\\opencv-python\\opencv\\modules\\imgcodecs\\src\\loadsave.cpp:993: error: (-215:Assertion failed) !buf.empty() in function 'cv::imdecode_'\n",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31merror\u001b[39m                                     Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 29\u001b[39m\n\u001b[32m     26\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m f \u001b[38;5;129;01min\u001b[39;00m file:\n\u001b[32m     27\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33m.jpg\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m f:\n\u001b[32m     28\u001b[39m \u001b[38;5;66;03m# read the image\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m29\u001b[39m         image = \u001b[43mcv2\u001b[49m\u001b[43m.\u001b[49m\u001b[43mimread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mos\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m.\u001b[49m\u001b[43mabspath\u001b[49m\u001b[43m(\u001b[49m\u001b[43mos\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m.\u001b[49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43mroot\u001b[49m\u001b[43m,\u001b[49m\u001b[43mf\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     30\u001b[39m         result_img, results = predict_and_detect(model, image, classes=[], conf=\u001b[32m0.5\u001b[39m)\n\u001b[32m     32\u001b[39m         \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mcoord_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mf\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.txt\u001b[39m\u001b[33m\"\u001b[39m,\u001b[33m'\u001b[39m\u001b[33ma\u001b[39m\u001b[33m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m file:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\fanti\\miniconda3\\envs\\tiamat_environnement\\Lib\\site-packages\\ultralytics\\utils\\patches.py:38\u001b[39m, in \u001b[36mimread\u001b[39m\u001b[34m(filename, flags)\u001b[39m\n\u001b[32m     36\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m     37\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m38\u001b[39m     im = \u001b[43mcv2\u001b[49m\u001b[43m.\u001b[49m\u001b[43mimdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile_bytes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mflags\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     39\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m im[..., \u001b[38;5;28;01mNone\u001b[39;00m] \u001b[38;5;28;01mif\u001b[39;00m im.ndim == \u001b[32m2\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m im\n",
      "\u001b[31merror\u001b[39m: OpenCV(4.11.0) D:\\a\\opencv-python\\opencv-python\\opencv\\modules\\imgcodecs\\src\\loadsave.cpp:993: error: (-215:Assertion failed) !buf.empty() in function 'cv::imdecode_'\n"
     ]
    }
   ],
   "source": [
    "def predict(chosen_model, img, classes=[], conf=0.5):\n",
    "    if classes:\n",
    "        results = chosen_model.predict(img, classes=classes, conf=conf)\n",
    "    else:\n",
    "        results = chosen_model.predict(img, conf=conf)\n",
    "\n",
    "    return results\n",
    "\n",
    "\n",
    "def predict_and_detect(chosen_model, img, classes=[], conf=0.5, rectangle_thickness=5, text_thickness=1):\n",
    "    results = predict(chosen_model, img, classes, conf=conf)\n",
    "    for result in results:\n",
    "        for box in result.boxes:\n",
    "            cv2.rectangle(img, (int(box.xyxy[0][0]), int(box.xyxy[0][1])),\n",
    "                          (int(box.xyxy[0][2]), int(box.xyxy[0][3])), (255, 0, 255), rectangle_thickness)\n",
    "            # cv2.putText(img, f\"{result.names[int(box.cls[0])]}\",\n",
    "            #             (int(box.xyxy[0][0]), int(box.xyxy[0][1]) - 10),\n",
    "            #             cv2.FONT_HERSHEY_PLAIN, 1, (255, 0, 0), text_thickness)\n",
    "    return img, results\n",
    "\n",
    "\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e735cc61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 640x480 10 photographs, 79.9ms\n",
      "Speed: 7.4ms preprocess, 79.9ms inference, 157.0ms postprocess per image at shape (1, 3, 640, 480)\n",
      "\n",
      "0: 640x480 5 photographs, 17.0ms\n",
      "Speed: 3.0ms preprocess, 17.0ms inference, 3.2ms postprocess per image at shape (1, 3, 640, 480)\n"
     ]
    }
   ],
   "source": [
    "model = YOLO(\"best.pt\")\n",
    "path=os.getcwd()+\"\\\\images_a_traiter\"\n",
    "for root, dirs, file in os.walk(path):\n",
    "    for f in file:\n",
    "        if \".jpg\" in f:\n",
    "    # read the image\n",
    "            image = cv2.imread(f\"images_a_traiter\\\\\"+f)\n",
    "            result_img, results = predict_and_detect(model, image, classes=[], conf=0.5)\n",
    "            \n",
    "            with open(f\"coord_{f}.txt\",'a') as file:\n",
    "                for result in results:\n",
    "                    df=result.to_df()\n",
    "                    df.to_csv(f\"{f}_annots.csv\")\n",
    "                    # for box in result.boxes:\n",
    "                    #      file.write(str(box)+\"\\n\")\n",
    "        # cv2.imshow(\"Image\", result_img)\n",
    "            cv2.imwrite(f\"result{f}.png\", result_img)\n",
    "            cv2.waitKey(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2b09eb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Read config file.\n",
    "config_path = \"pero_eu_cz_print_newspapers_2022-09-26\\\\config.ini\"\n",
    "config = configparser.ConfigParser()\n",
    "config.read(config_path)\n",
    "\n",
    "# # Init the OCR pipeline. \n",
    "# # You have to specify config_path to be able to use relative paths\n",
    "# # inside the config file.\n",
    "page_parser = PageParser(config, config_path=os.path.dirname(config_path))\n",
    "\n",
    "\n",
    "# Read the document page image.\n",
    "path=os.getcwd()+\"\\\\images_a_traiter\"\n",
    "for f in os.listdir(path):\n",
    "        \n",
    "            \n",
    "            input_image_path = f\"images_a_traiter\\\\{f}\"\n",
    "            image = cv2.imread(input_image_path, 1)\n",
    "\n",
    "            # Init empty page content. \n",
    "            # This object will be updated by the ocr pipeline. id can be any string and it is used to identify the page.\n",
    "            page_layout = PageLayout(id=input_image_path, page_size=(image.shape[0], image.shape[1]))\n",
    "\n",
    "        # Process the image by the OCR pipeline\n",
    "        # try:\n",
    "            page_layout = page_parser.process_page(image, page_layout)\n",
    "\n",
    "\n",
    "            page_layout.to_pagexml(f'dossier_output_experience\\\\output_page{f}.xml') # Save results as Page XML.\n",
    "            page_layout.to_altoxml(f'dossier_output_experience\\\\output_ALTO{f}.xml') # Save results as ALTO XML.\n",
    "            for region in page_layout.regions:\n",
    "                \n",
    "                liste_lignes = [f\"Zone_texte\",]\n",
    "\n",
    "                for line in region.lines:\n",
    "                    liste_lignes.append(line.transcription)\n",
    "\n",
    "                with open(f'dossier_output_experience\\\\texte{f}.txt', 'a', encoding='utf-8') as txt:\n",
    "                    txt.write('\\n')\n",
    "                    for element in liste_lignes:\n",
    "                        txt.write(element +\" \")\n",
    "        # except:\n",
    "        #     print(f\"probleme avec {f}\")\n",
    "        #     pass\n",
    "\n",
    "        # Render detected text regions and text lines into the image and\n",
    "        # save it into a file.\n",
    "            rendered_image = page_layout.render_to_image(image) \n",
    "            cv2.imwrite(f'results_full2\\\\{f}.png', rendered_image)\n",
    "\n",
    "            # Save each cropped text line in a separate .jpg file.\n",
    "            # for region in page_layout.regions:\n",
    "            #     for line in region.lines:\n",
    "            #         cv2.imwrite(f'file_id-{line.id}.jpg', line.crop.astype(np.uint8))\n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tiamat_environnement",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
