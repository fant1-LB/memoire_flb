{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4b614ca9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\fanti\\miniconda3\\envs\\tiamat_environnement\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import cv2\n",
    "from transformers import CLIPProcessor, CLIPModel\n",
    "import os\n",
    "import ast\n",
    "from PIL import Image\n",
    "from deep_translator import GoogleTranslator, DeeplTranslator\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14cb89dd",
   "metadata": {},
   "source": [
    "## Extraction des images détectées par le Yolo dans des fichiers séparés ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d2768603",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_images(fichier, chemin_image):\n",
    "    df=pd.read_csv(fichier)\n",
    "    liste_coords =df['box'].to_list()\n",
    "        \n",
    "    path=os.getcwd()\n",
    "    \n",
    "    image = cv2.imread(path + \"\\\\\"+ chemin_image)\n",
    "    \n",
    "    if image is None:\n",
    "            print(f\"Erreur : impossible de charger l'image {chemin_image}\")\n",
    "            exit()\n",
    "    nom_dossier = chemin_image.replace(\".jpg\",\"\").replace('images_a_traiter\\\\','')\n",
    "    print(nom_dossier)\n",
    "    if not os.path.exists(path+\"\\\\\"+\"sorties\"+\"\\\\\"+nom_dossier):\n",
    "        os.makedirs(\"sorties\\\\\"+nom_dossier)\n",
    "        print(f\"{nom_dossier} créé\")\n",
    "    compteur=0\n",
    "    for i in liste_coords:\n",
    "        compteur+=1\n",
    "        dico = ast.literal_eval(i)\n",
    "        valeurs=list(dico.values())\n",
    "        \n",
    "        y1, x1, y2, x2 = int(round(valeurs[0])), int(round(valeurs[1])), int(round(valeurs[2])), int(round(valeurs[3]))\n",
    "        cropped_img= image[x1:x2,y1:y2]\n",
    "        cv2.imwrite(f\"sorties\\\\{nom_dossier}\\\\{nom_dossier}_illus{compteur}.png\", cropped_img)\n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c750042d",
   "metadata": {},
   "outputs": [],
   "source": [
    "path=os.getcwd()+\"\\\\images_a_traiter\"\n",
    "for root, dirs, file in os.walk(path):\n",
    "    for image in file:\n",
    "        if \".jpg\" in image:\n",
    "            nom_csv = image+\"_annots.csv\"\n",
    "            chemin_image = \"images_a_traiter\\\\\"+image\n",
    "            extract_images(nom_csv, chemin_image)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bfb4bec",
   "metadata": {},
   "source": [
    "Fonction optionnelle pour afficher les images d'une page sous forme de grille, empruntée à HALUKPA Kerry, \"Getting started with OpenAI's CLIP\", sur Medium, 2023, consulté le 27/08/2025.\n",
    "https://medium.com/@kerry.halupka/getting-started-with-openais-clip-a3b8f5277867"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3930230f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def image_grid(imgs, cols):\n",
    "    rows = (len(imgs) + cols - 1) // cols\n",
    "    w, h = imgs[0].size\n",
    "    grid = Image.new('RGB', size=(cols*w, rows*h))\n",
    "    \n",
    "    for i, img in enumerate(imgs):\n",
    "        grid.paste(img, box=(i%cols*w, i//cols*h))\n",
    "    return grid\n",
    "\n",
    "\n",
    "def montrer_images(nom_image):\n",
    "    path=os.getcwd()\n",
    "\n",
    "    image_urls=[]\n",
    "    for i in range(1,20):\n",
    "        image_urls.append(str(path+\"\\\\\"+\"sorties\"+\"\\\\\"+nom_image+\"_illus\"+str(i)+\".png\"))\n",
    "    images = []\n",
    "    try:\n",
    "        for url in image_urls:\n",
    "            images.append(Image.open(url))\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    grid = image_grid(images, cols=3)\n",
    "    display(grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1c4ecdf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clip_integral():\n",
    "    liste_images = obtenir_liste_images()\n",
    "    for i in liste_images:\n",
    "        liste2=obtenir_liste_captions(i)\n",
    "        liste3 = traduire(liste2)\n",
    "        \n",
    "        application_CLIP(liste3, i, liste2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c01fefd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def obtenir_liste_images():\n",
    "    path=os.getcwd()+\"\\\\sorties\"\n",
    "    liste_images = []\n",
    "    for root, dirs, files in os.walk(path):\n",
    "            \n",
    "        for dir in dirs:\n",
    "                \n",
    "            liste_images.append(str(dir))\n",
    "    return(liste_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "acde39b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def obtenir_liste_captions(nom_image):\n",
    "   fichier_texte=str('dossier_output_experience\\\\texte'+nom_image+\".jpg.txt\") #modifier le .jpg selon le format de l'image de la page de journal originale\n",
    "   with open (fichier_texte,'r', encoding=\"utf-8\") as ft:\n",
    "         texte=ft.read()\n",
    "         liste_zones=texte.split(\"Zone_texte\")\n",
    "         \n",
    "         liste2=[]\n",
    "         for i in range(0, len(liste_zones)):\n",
    "            \n",
    "            if len(liste_zones[i])>= 10 and len(liste_zones[i])<=77:\n",
    "               terme=liste_zones[i].replace(\"\\n\",\"\")\n",
    "               terme=liste_zones[i].replace(\"- \",\"\")\n",
    "               liste2.append(terme)\n",
    "   return(liste2)       \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5d567f29",
   "metadata": {},
   "outputs": [],
   "source": [
    "def traduire(liste2):\n",
    "    # liste3= DeeplTranslator(api_key=\" INPUT YOUR OWN API KEY\", source=\"fr\", target=\"en\", use_free_api=True).translate_batch(liste2)\n",
    "    liste3= GoogleTranslator(\"fr\",\"en\").translate_batch(liste2)\n",
    "    return(liste3)\n",
    "#Cellule pour passer les textes en anglais, CLIP ayant été entraîné sur des textes en anglais."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "143c453f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def application_CLIP(liste3, dossier, liste2):\n",
    "    model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "    processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "    images=[]\n",
    "    path= os.getcwd()+\"\\\\sorties\"+\"\\\\\"+dossier\n",
    "    for root, dirs, files in os.walk(path):\n",
    "        for image in files:\n",
    "            if \".png\" in image:\n",
    "                image= Image.open(path+\"\\\\\"+image)\n",
    "                images.append(image)\n",
    "        \n",
    "    inputs = processor(text=liste3, images=images, return_tensors=\"pt\", padding=True, do_rescale= True, do_convert_rgb=False)\n",
    "    outputs = model(**inputs)\n",
    "    logits_per_image = outputs.logits_per_image  # this is the image-text similarity score\n",
    "    probs = logits_per_image.softmax(dim=1)  # we can take the softmax to get the label probabilities\n",
    "    liste_probas =probs.tolist() \n",
    "    \n",
    "    liste_probas_par_ordre_images=[]\n",
    "    for i in range(len(liste_probas)):\n",
    "        indices_probas = []\n",
    "        for idx, probab in enumerate(liste_probas[i]):  # Use enumerate to get index\n",
    "            if probab >= 0.2:\n",
    "                indices_probas.append(idx)  # Use the actual index from enumerate\n",
    "        liste_probas_par_ordre_images.append(indices_probas)\n",
    "    compteur=1\n",
    "    print(liste_probas_par_ordre_images)\n",
    "    for i in liste_probas_par_ordre_images:\n",
    "        with open(f\"{dossier}.txt\",\"a\")as file:\n",
    "            file.write(f\"image n{compteur}\\n\")\n",
    "            for y in i:\n",
    "                file.write(liste2[y])\n",
    "                \n",
    "        compteur+=1\n",
    "            \n",
    "        \n",
    "    return(probs, liste_probas_par_ordre_images)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cbbc8ea1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[9, 28], [9, 22], [9], [9, 22], [9], [9], [9, 22], [9], [9, 28], [9, 28]]\n",
      "[[12], [12], [10, 12], [1, 14], [1]]\n"
     ]
    }
   ],
   "source": [
    "clip_integral()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tiamat_environnement",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
