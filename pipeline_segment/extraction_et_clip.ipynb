{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4b614ca9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import cv2\n",
    "from transformers import CLIPProcessor, CLIPModel\n",
    "import os\n",
    "import ast\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d2768603",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_images(fichier, image):\n",
    "    df=pd.read_csv(fichier)\n",
    "    liste_coords =df['box'].to_list()\n",
    "    nom_image = image\n",
    "    img = nom_image+\".jpg\"\n",
    "    path=os.getcwd()\n",
    "    img = cv2.imread(image)\n",
    "    image = cv2.imread(path + \"\\\\\"+ img)\n",
    "    \n",
    "    if image is None:\n",
    "            print(f\"Erreur : impossible de charger l'image {img}\")\n",
    "            exit()\n",
    "    if not os.path.exists(path+\"\\\\\"+nom_image):\n",
    "        os.makedirs(nom_image)\n",
    "    compteur=0\n",
    "    for i in liste_coords:\n",
    "        compteur+=1\n",
    "        dico = ast.literal_eval(i)\n",
    "        valeurs=list(dico.values())\n",
    "        print(valeurs)\n",
    "        y1, x1, y2, x2 = int(round(valeurs[0])), int(round(valeurs[1])), int(round(valeurs[2])), int(round(valeurs[3]))\n",
    "        print(x1, x2, y1, y2)\n",
    "        cropped_img= image[x1:x2,y1:y2 ]\n",
    "        cv2.imwrite(f\"{nom_image}\\\\{nom_image}_illus{compteur}.png\", cropped_img)\n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8e2d78f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"{'x1': 1115.96655, 'y1': 6915.6626, 'x2': 3418.42432, 'y2': 8769.93262}\", \"{'x1': 1148.16565, 'y1': 4820.76221, 'x2': 1990.17798, 'y2': 5961.53662}\", \"{'x1': 4787.70996, 'y1': 2563.77197, 'x2': 5679.35889, 'y2': 3409.8147}\", \"{'x1': 1163.9165, 'y1': 2529.18872, 'x2': 2029.17322, 'y2': 3662.80566}\", \"{'x1': 4794.29443, 'y1': 5110.67822, 'x2': 5669.86572, 'y2': 5952.92725}\", \"{'x1': 4802.31592, 'y1': 3423.83398, 'x2': 5656.80615, 'y2': 4252.19482}\", \"{'x1': 1151.4491, 'y1': 3676.41309, 'x2': 2016.38782, 'y2': 4792.65479}\", \"{'x1': 3446.02905, 'y1': 6897.74268, 'x2': 5615.78564, 'y2': 7813.39795}\", \"{'x1': 3438.11816, 'y1': 7827.729, 'x2': 5634.24121, 'y2': 8760.6875}\", \"{'x1': 4789.72705, 'y1': 4239.82959, 'x2': 5676.2749, 'y2': 5112.90869}\"]\n",
      "c:\\Users\\fanti\\Documents\\stage_highvision\\retrainyolomaison\\tests\\testphase2\\clip_and_co\\Excelsior19240710image1.jpg\n",
      "[1115.96655, 6915.6626, 3418.42432, 8769.93262]\n",
      "6916 8770 1116 3418\n",
      "[1148.16565, 4820.76221, 1990.17798, 5961.53662]\n",
      "4821 5962 1148 1990\n",
      "[4787.70996, 2563.77197, 5679.35889, 3409.8147]\n",
      "2564 3410 4788 5679\n",
      "[1163.9165, 2529.18872, 2029.17322, 3662.80566]\n",
      "2529 3663 1164 2029\n",
      "[4794.29443, 5110.67822, 5669.86572, 5952.92725]\n",
      "5111 5953 4794 5670\n",
      "[4802.31592, 3423.83398, 5656.80615, 4252.19482]\n",
      "3424 4252 4802 5657\n",
      "[1151.4491, 3676.41309, 2016.38782, 4792.65479]\n",
      "3676 4793 1151 2016\n",
      "[3446.02905, 6897.74268, 5615.78564, 7813.39795]\n",
      "6898 7813 3446 5616\n",
      "[3438.11816, 7827.729, 5634.24121, 8760.6875]\n",
      "7828 8761 3438 5634\n",
      "[4789.72705, 4239.82959, 5676.2749, 5112.90869]\n",
      "4240 5113 4790 5676\n"
     ]
    }
   ],
   "source": [
    "df= pd.read_csv('C:\\\\Users\\\\fanti\\\\Documents\\\\stage_highvision\\\\retrainyolomaison\\\\tests\\\\testphase2\\\\demonstration\\\\Excelsior19240710image1.jpg_annots.csv')\n",
    "\n",
    "liste_coords =df['box'].to_list()\n",
    "print(liste_coords)\n",
    "path=os.getcwd()\n",
    "nom_image =\"Excelsior19240710image1\"\n",
    "img = nom_image+\".jpg\"\n",
    "print(path + \"\\\\\"+ img)\n",
    "image = cv2.imread(path + \"\\\\\"+ img)\n",
    "if image is None:\n",
    "    print(f\"Erreur : impossible de charger l'image {nom_image}\")\n",
    "    exit()\n",
    "compteur=0\n",
    "if not os.path.exists(path+\"\\\\\"+nom_image):\n",
    "    os.makedirs(nom_image)\n",
    "for i in liste_coords:\n",
    "    compteur+=1\n",
    "    dico = ast.literal_eval(i)\n",
    "    valeurs=list(dico.values())\n",
    "    print(valeurs)\n",
    "    y1, x1, y2, x2 = int(round(valeurs[0])), int(round(valeurs[1])), int(round(valeurs[2])), int(round(valeurs[3]))\n",
    "    print(x1, x2, y1, y2)\n",
    "    cropped_img= image[x1:x2,y1:y2 ]\n",
    "    cv2.imwrite(f\"{nom_image}\\\\{nom_image}_illus{compteur}.png\", cropped_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d215ca46",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n"
     ]
    }
   ],
   "source": [
    "model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bfb4bec",
   "metadata": {},
   "source": [
    "https://medium.com/@kerry.halupka/getting-started-with-openais-clip-a3b8f5277867"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3930230f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def image_grid(imgs, cols):\n",
    "    rows = (len(imgs) + cols - 1) // cols\n",
    "    w, h = imgs[0].size\n",
    "    grid = Image.new('RGB', size=(cols*w, rows*h))\n",
    "    \n",
    "    for i, img in enumerate(imgs):\n",
    "        grid.paste(img, box=(i%cols*w, i//cols*h))\n",
    "    return grid\n",
    "path=os.getcwd()\n",
    "nom_image=\"Excelsior19240710image1\"\n",
    "image_urls=[]\n",
    "for i in range(1,20):\n",
    "    image_urls.append(str(path+\"\\\\\"+nom_image+\"\\\\\"+nom_image+\"_illus\"+str(i)+\".png\"))\n",
    "images = []\n",
    "try:\n",
    "    for url in image_urls:\n",
    "        images.append(Image.open(url))\n",
    "except:\n",
    "    pass\n",
    "\n",
    "grid = image_grid(images, cols=3)\n",
    "display(grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acde39b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "fichier_texte=str('texte'+nom_image+\".txt\")\n",
    "with open (fichier_texte,'r', encoding=\"utf-8\") as ft:\n",
    "   texte=ft.read()\n",
    "   liste_zones=texte.split(\"ZONE_TEXTE\")\n",
    "   \n",
    "   liste2=[]\n",
    "   for i in range(0, len(liste_zones)):\n",
    "      \n",
    "      if len(liste_zones[i])>= 10 and len(liste_zones[i])<=77:\n",
    "         terme=liste_zones[i].replace(\"\\n\",\"\")\n",
    "         terme=liste_zones[i].replace(\"- \",\"\")\n",
    "         liste2.append(terme)\n",
    "print(liste2)       \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d567f29",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'liste2' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m#cellule de traduction\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mdeep_translator\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m GoogleTranslator\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m liste3= GoogleTranslator(\u001b[33m\"\u001b[39m\u001b[33mfr\u001b[39m\u001b[33m\"\u001b[39m,\u001b[33m\"\u001b[39m\u001b[33men\u001b[39m\u001b[33m\"\u001b[39m).translate_batch(\u001b[43mliste2\u001b[49m)\n",
      "\u001b[31mNameError\u001b[39m: name 'liste2' is not defined"
     ]
    }
   ],
   "source": [
    "#cellule de traduction\n",
    "from deep_translator import GoogleTranslator\n",
    "liste3= GoogleTranslator(\"fr\",\"en\").translate_batch(liste2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "321e3980",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = processor(text=liste3, images=images, return_tensors=\"pt\", padding=True, do_convert_rgb=False)\n",
    "\n",
    "outputs = model(**inputs)\n",
    "logits_per_image = outputs.logits_per_image  # this is the image-text similarity score\n",
    "probs = logits_per_image.softmax(dim=1)  # we can take the softmax to get the label probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "516d0445",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig = plt.figure(figsize=(8, 20))\n",
    "\n",
    "for idx in range(len(images)):\n",
    "\n",
    "    # show original image\n",
    "    fig.add_subplot(len(images), 2, 2*(idx+1)-1 )\n",
    "    plt.imshow(images[idx])\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "\n",
    "    # show probabilities\n",
    "    fig.add_subplot(len(images), 2, 2*(idx+1))\n",
    "    \n",
    "    plt.barh(range(len(probs[0].detach().numpy())),probs[idx].detach().numpy(), tick_label=liste2)\n",
    "    plt.xlim(0,1.0)\n",
    "\n",
    "    plt.subplots_adjust(left=0.1,\n",
    "                        bottom=0.1,\n",
    "                        right=0.9,\n",
    "                        top=0.9,\n",
    "                        wspace=0.2,\n",
    "                        hspace=0.8)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "330b66ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(probs)\n",
    "liste_probas =probs.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f60bd31d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print (liste_probas)\n",
    "for i in liste_probas:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1922272f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[9]\n"
     ]
    }
   ],
   "source": [
    "probabs_image1=[]\n",
    "for probab in liste_probas[0]:\n",
    "    if probab>=0.4:\n",
    "        probabs_image1.append(liste_probas[0].index(probab))\n",
    "print(probabs_image1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81a3371e",
   "metadata": {},
   "outputs": [],
   "source": [
    "liste_probas_par_ordre_images=[]\n",
    "\n",
    "for i in range(0,len(liste_probas)):\n",
    "    indices_probas=[]\n",
    "    for probab in liste_probas[i]:\n",
    "        if probab>=0.2:\n",
    "            indices_probas.append(liste_probas[i].index(probab))\n",
    "    liste_probas_par_ordre_images.append(indices_probas)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "794357d3",
   "metadata": {},
   "source": [
    "Dans la cellule ci dessous, l'indice correspond au numéro de l'image, les chiffres à l'indice dans la liste des morceaux de texte des textes associés à plus de 0,2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3b987b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(liste_probas_par_ordre_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a37d3164",
   "metadata": {},
   "outputs": [],
   "source": [
    "compteur=1\n",
    "for i in liste_probas_par_ordre_images:\n",
    "    print(f\"image n{compteur}\")\n",
    "    compteur+=1\n",
    "    for y in i:\n",
    "        print(liste2[y])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tiamat_environnement",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
