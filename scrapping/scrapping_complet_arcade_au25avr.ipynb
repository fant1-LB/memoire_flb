{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65be983d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup as bs\n",
    "import requests\n",
    "import os\n",
    "import tqdm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7866426",
   "metadata": {},
   "outputs": [],
   "source": [
    "url_0 = \"http://www2.culture.gouv.fr/documentation/arcade/salons.htm\"\n",
    "reponse_0 = requests.get(url_0)\n",
    "reponse_0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b14b1e7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "if str(reponse_0) == \"<Response [200]>\":\n",
    "    contenu_0= reponse_0.content\n",
    "    soup_0 = bs(contenu_0, \"html.parser\")\n",
    "    liste_0=soup_0.ul\n",
    "    liens=[]\n",
    "    for a in liste_0.find_all('a'):\n",
    "        lien=a.get(\"href\")\n",
    "        liens.append(lien)\n",
    "    print (liens)\n",
    "else:\n",
    "    print(f\"{url_0} ne réponds pas\")\n",
    "        \n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bf230b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_dossier= 0\n",
    "constante = \"http://www2.culture.gouv.fr\"\n",
    "liens_notices=[]\n",
    "for i in liens:\n",
    "    num_dossier+=1\n",
    "    url=str(i)\n",
    "    reponse_1=requests.get(url)\n",
    "    chemin = f\"C:\\\\Users\\\\Fantin.LEBER\\\\Documents\\\\stage_higvision\\\\scrapping\\\\album{num_dossier}\"\n",
    "    contenu_1=reponse_1.content\n",
    "    if str(reponse_1) == \"<Response [200]>\":\n",
    "        if os.path.exists(chemin):\n",
    "            pass\n",
    "        else:\n",
    "            os.makedirs(chemin)\n",
    "        soup=bs(contenu_1, \"html.parser\")\n",
    "        \n",
    "        liens_notices=[]\n",
    "        for a in soup.find_all('a'):\n",
    "            lien = a.get(\"href\")\n",
    "            liens_notices.append(lien)\n",
    "        if len(liens_notices)>= 205:\n",
    "            lien_page_2 = liens_notices[-1]\n",
    "            requete_page_2= requests.get(constante+lien_page_2)\n",
    "            contenu_page2= requete_page_2.content\n",
    "            soup2 = bs(contenu_page2, \"html.parser\")\n",
    "            for a in soup2.find_all('a'):\n",
    "                lien = a.get(\"href\")\n",
    "                liens_notices.append(lien)\n",
    "            with open(f\"C:\\\\Users\\\\Fantin.LEBER\\\\Documents\\\\stage_higvision\\\\scrapping\\\\album{num_dossier}\\\\fichier{num_dossier}.txt\",'w')as fichier:\n",
    "                compteur = 0\n",
    "                for i in liens_notices:\n",
    "                 compteur +=1\n",
    "                 fichier.write(f\" {i}\")\n",
    "\n",
    "        else:\n",
    "            with open(f\"C:\\\\Users\\\\Fantin.LEBER\\\\Documents\\\\stage_higvision\\\\scrapping\\\\album{num_dossier}\\\\fichier{num_dossier}.txt\",'w') as fichier:\n",
    "                compteur = 0\n",
    "                for i in liens_notices:\n",
    "                 compteur +=1\n",
    "                 fichier.write(f\" {i}\")\n",
    "\n",
    "\n",
    "    else:\n",
    "        print (f\"{url} ne réponds pas\")\n",
    "        pass\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b79f6d70",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "# print(liens_notices)\n",
    "# len(liens_notices)\n",
    "for x in range (1,34):\n",
    "   liens_notices_propres = []\n",
    "   with open(f\"C:\\\\Users\\\\Fantin.LEBER\\\\Documents\\\\stage_higvision\\\\scrapping\\\\album{x}\\\\fichier{x}.txt\",'r',encoding=\"utf8\") as fichier:\n",
    "     \n",
    "      contenu = fichier.read()\n",
    "      print(contenu)\n",
    "      \n",
    "      liens_notices= str(contenu).split()\n",
    "      print(liens_notices)\n",
    "      for i in liens_notices:\n",
    "        if re.match(r\".*ACTION=RETROUVER&FIELD.*\",i):\n",
    "            liens_notices_propres.append(i)\n",
    "            print (liens_notices_propres)\n",
    "            with open(f\"C:\\\\Users\\\\Fantin.LEBER\\\\Documents\\\\stage_higvision\\\\scrapping\\\\album{x}\\\\notices_propres{x}.txt\",'w') as fichier:\n",
    "               compteur=0\n",
    "               for i in liens_notices_propres:\n",
    "                  compteur+=1\n",
    "                  fichier.write(f\" {i}\")\n",
    "        else:\n",
    "           pass\n",
    "          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1037208",
   "metadata": {},
   "outputs": [],
   "source": [
    "for x in range (1,34):\n",
    "   liens_notices_propres = []\n",
    "   with open(f\"C:\\\\Users\\\\Fantin.LEBER\\\\Documents\\\\stage_higvision\\\\scrapping\\\\album{x}\\\\notices_propres{x}.txt\",'r',encoding=\"utf8\") as fichier:\n",
    "     \n",
    "      contenu = fichier.read()\n",
    "      \n",
    "      liens_notices_propres = str(contenu).split()\n",
    "      # print(liens_notices)\n",
    "   liens_def = []\n",
    "   for i in liens_notices_propres:\n",
    "      i2 = constante+str(i)\n",
    "      liens_def.append(i2)\n",
    "      with open(f\"C:\\\\Users\\\\Fantin.LEBER\\\\Documents\\\\stage_higvision\\\\scrapping\\\\album{x}\\\\notices_lien_def{x}.txt\",'w') as fichier:\n",
    "               # compteur=0\n",
    "               for y in liens_def:\n",
    "                  # compteur+=1\n",
    "                  fichier.write(f\" {y}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8929eaf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for x  in range (1,34):\n",
    "#    with open(f\"C:\\\\Users\\\\Fantin.LEBER\\\\Documents\\\\stage_higvision\\\\scrapping\\\\album{x}\\\\notices_lien_def{x}.txt\",'r',encoding=\"utf8\") as fichier:\n",
    "     \n",
    "#       contenu = fichier.read()\n",
    "      \n",
    "#       liens_notices_finaux = str(contenu).split() \n",
    "#       for z in liens_notices_finaux:\n",
    "         # requete=requests.get(z)\n",
    "         # contenu = requete.content\n",
    "         # soup=bs(contenu, \"html.parser\")\n",
    "         # liste_etiquettes =soup.find_all('td', align=\"RIGHT\", width=\"150\")\n",
    "         # liste_infos=soup.find_all('td', width=\"500\", bgcolor=\"#FFFFFF\")\n",
    "         # # print (liste_etiquettes)\n",
    "         # print(liste_infos)\n",
    "         # liste_infos_str =[]\n",
    "         # for i in liste_infos:\n",
    "         #    i2= str(i)\n",
    "         #    liste_infos_str.append(i2)\n",
    "         # # print (liste_infos_str)\n",
    "         # liste_infos_clean=[]\n",
    "         # for i in liste_infos_str:\n",
    "         #    i=i.replace('<td bgcolor=\"#FFFFFF\" width=\"500\">',\"\")\n",
    "         #    i=i.replace('<font color=\"#000000\">','')\n",
    "         #    i=i.replace('</font>','')\n",
    "         #    i=i.replace('</td>, <td bgcolor=\"#FFFFFF\" width=\"500\">','')\n",
    "         #    i=i.replace('\\\\n<b>','')\n",
    "         #    i=i.replace('</b>','')\n",
    "         #    i=i.replace('<n>','')\n",
    "         #    i=i.replace('</td>','')\n",
    "         #    i=i.replace('</n>','')\n",
    "         #    i=i.replace('<b>','')\n",
    "         #    i = i.replace('(<a href=\"/public/mistral/arcade_fr?ACTION=CHERCHER&amp;FIELD_98=AUTR&amp;VALUE_98=M&amp;DOM=Tous&amp;REL_SPECIFIC=3\">M</a>.)',\"\")\n",
    "         #    liste_infos_clean.append(i)\n",
    "         \n",
    "         # liste_etiquettes_str =[]\n",
    "         # for i in liste_etiquettes:\n",
    "         #    i2= str(i)\n",
    "         #    liste_etiquettes_str.append(i2)\n",
    "         # # print (liste_infos_str)\n",
    "         # liste_etiquettes_clean=[]\n",
    "         # for i in liste_etiquettes_str:\n",
    "         #    i=i.replace('<td bgcolor=\"#FFFFFF\" width=\"500\">',\"\")\n",
    "         #    i=i.replace('<font color=\"#000000\">','')\n",
    "         #    i=i.replace('</font>','')\n",
    "         #    i=i.replace('</td>, <td bgcolor=\"#FFFFFF\" width=\"500\">','')\n",
    "         #    i=i.replace('\\\\n<b>','')\n",
    "         #    i=i.replace('</b>','')\n",
    "         #    i=i.replace('<n>','')\n",
    "         #    i=i.replace('</td>','')\n",
    "         #    i=i.replace('</n>','')\n",
    "         #    i=i.replace('<b>','')\n",
    "         #    i=i.replace('<td align=\"RIGHT\" width=\"150\">','')\n",
    "         #    i=i.replace('<font color=\"#003366\">','')\n",
    "         #    i = i.replace('(<a href=\"/public/mistral/arcade_fr?ACTION=CHERCHER&amp;FIELD_98=AUTR&amp;VALUE_98=M&amp;DOM=Tous&amp;REL_SPECIFIC=3\">M</a>.)',\"\")\n",
    "         #    liste_etiquettes_clean.append(i)\n",
    "         \n",
    "         # tuples_total = list(zip(liste_etiquettes_clean, liste_infos_clean))\n",
    "         # # print(tuples_total)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c69de27",
   "metadata": {},
   "outputs": [],
   "source": [
    "x=1\n",
    "with open(f\"C:\\\\Users\\\\Fantin.LEBER\\\\Documents\\\\stage_higvision\\\\scrapping\\\\album{x}\\\\notices_lien_def{x}.txt\",'r',encoding=\"utf8\") as fichier:\n",
    "     \n",
    "      contenu = fichier.read()\n",
    "      \n",
    "      liens_notices_finaux = str(contenu).split() \n",
    "      compteur=0\n",
    "      for z in liens_notices_finaux:\n",
    "         compteur+=1\n",
    "         requete=requests.get(z)\n",
    "         contenu = requete.content\n",
    "         soup=bs(contenu, \"html.parser\")\n",
    "         infos_triees =soup.find_all('n')\n",
    "         # numero =soup.find('b')\n",
    "         with open (f\"C:\\\\Users\\\\Fantin.LEBER\\\\Documents\\\\stage_higvision\\\\scrapping\\\\album{x}\\\\resultat{compteur}.txt\",'w',encoding=\"utf8\") as fichier:\n",
    "            fichier.write(str(infos_triees))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb5365ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for x in range (1,300):\n",
    "#     try:\n",
    "#         with open (f\"C:\\\\Users\\\\Fantin.LEBER\\\\Documents\\\\stage_higvision\\\\scrapping\\\\album1\\\\resultat{x}.txt\",'w',encoding=\"utf8\") as fichier:\n",
    "            \n",
    "#     except:\n",
    "#         pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0f22daf",
   "metadata": {},
   "outputs": [],
   "source": [
    "for x in range(1,34):\n",
    "  with open(f\"C:\\\\Users\\\\Fantin.LEBER\\\\Documents\\\\stage_higvision\\\\scrapping\\\\album{x}\\\\notices_lien_def{x}.txt\",'r',encoding=\"utf8\") as fichier:\n",
    "      contenu = fichier.read()\n",
    "      liens_notices_finaux = str(contenu).split()\n",
    "      # print(liens_notices_finaux)\n",
    "      liens_notices_uniques=[]\n",
    "      for i in liens_notices_finaux:\n",
    "         if i in liens_notices_uniques:\n",
    "            pass\n",
    "         else:\n",
    "            liens_notices_uniques.append(i)\n",
    "      compteur=0\n",
    "      datafull=[]\n",
    "      for z in liens_notices_uniques:\n",
    "            # print(z)\n",
    "            data=[]\n",
    "            response = requests.get(z)\n",
    "            response.encoding = 'ISO-8859-1'  # Très important pour les accents\n",
    "            reponse=response.content\n",
    "            soup = bs(reponse, 'html.parser')\n",
    "            table = soup.find('table', valign='TOP' )  \n",
    "            if table:\n",
    "                 with open(f\"C:\\\\Users\\\\Fantin.LEBER\\\\Documents\\\\stage_higvision\\\\scrapping_def\\\\resultat{x}_et_{compteur}.txt\",'w',encoding=\"utf8\") as result:\n",
    "                  compteur+=1\n",
    "                  rows = table.find_all('tr')\n",
    "                  # print(rows)\n",
    "                  for row in rows:\n",
    "                     result.write(row.get_text(strip=True, separator=';'))\n",
    "                  #   data.append(row.get_text(strip=True, separator=';')) \n",
    "                  #   result.write(str(data))\n",
    "                  #   datafull.append(data)    \n",
    "            with open(f\"C:\\\\Users\\\\Fantin.LEBER\\\\Documents\\\\stage_higvision\\\\scrapping_def\\\\resultatfull_{x}.txt\",'w',encoding=\"utf8\") as result_full:\n",
    "             result_full.write(row.get_text(strip=True, separator=';'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98313405",
   "metadata": {},
   "outputs": [],
   "source": [
    "print (data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d3ae440",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import os\n",
    "balises = [\"Cote\",\"Auteur\",\"Dénomination\",\"Technique\",\"Matériau\",\"Titre\",\"Thème représenté\",\"Mot-clé\",\"Stade de la création\",\"Exposition\",\"Nature de l'acte\",\"Procédure\",\"Date de l'acte\",\"Provenance\",\"Commentaires\",\"Localisation\",\"Édifice\",\"Nom de l'édifice\",\"voir aussi\",\"Copyright\",\" N° notice\"]\n",
    "\n",
    "for i in range(1,34):\n",
    "    grosdico=[]\n",
    "    # os.makedirs(f\"C:\\\\Users\\\\Fantin.LEBER\\\\Documents\\\\stage_higvision\\\\scrapping_def\\\\scrap_final{i}\")\n",
    "    for z in range (1,200):\n",
    "        try:\n",
    "        \n",
    "            dico={}\n",
    "            with open(f\"C:\\\\Users\\\\fanti\\\\Documents\\\\stage_highvision\\\\stage_higvision(1)\\\\stage_higvision(2)\\\\stage_higvision\\\\scrapping_def\\\\resultat{i}_et_{z}.txt\",'r',encoding=\"utf8\") as fichier:\n",
    "                infos=fichier.read()\n",
    "                # print (infos)\n",
    "                # os.makedirs(f\"C:\\\\Users\\\\Fantin.LEBER\\\\Documents\\\\stage_higvision\\\\scrapping_def\\\\scrap_final{i}\")\n",
    "                data= infos.split(\";\")\n",
    "                \n",
    "                # print(data)\n",
    "                # print(data)\n",
    "                balises_y = []\n",
    "                for y in range(0,(len(data))):\n",
    "                    # print(len(data))\n",
    "                    \n",
    "                    # print(data[y])\n",
    "                    for avion in balises:\n",
    "                        # print(avion)\n",
    "                        if str(data[y])== str(avion):\n",
    "                            balises_y.append(y)\n",
    "                        # else:\n",
    "                        #     pass\n",
    "                    # print(balises_y)\n",
    "                    \n",
    "                for w in range(0,(len(balises_y))):\n",
    "                    # print(len(balises_y))        \n",
    "                    try:\n",
    "                        infos=[]\n",
    "                        for position in range ((balises_y[w]),(balises_y[w+1])):\n",
    "                                infos.append(data[position])\n",
    "                       \n",
    "                        for jpp in range (1, len(infos)):\n",
    "                            for youpi in balises:\n",
    "                                if infos[jpp]==youpi:\n",
    "                                    infos.pop(jpp)\n",
    "                    except:\n",
    "                      pass\n",
    "                    dico[str(data[int(balises_y[int(w)])])]= \" \".join(str(element) for element in infos)\n",
    "                # print(dico)\n",
    "                grosdico.append(dico)\n",
    "                # print(grosdico)\n",
    "                          \n",
    "                        # print(dico)\n",
    "                with open (f\"C:\\\\Users\\\\fanti\\\\Documents\\\\stage_highvision\\\\stage_higvision(1)\\\\stage_higvision(2)\\\\stage_higvision\\\\scrapping_def\\\\scrap_final{i}\\\\final_v3_{i}.csv\",\"w\",newline=\"\") as final:\n",
    "                            writer = csv.DictWriter(final, fieldnames=balises)\n",
    "                            writer.writeheader()\n",
    "                            writer.writerows(grosdico)\n",
    "                            print(f\"final_v3_{i} créé\") \n",
    "        except:\n",
    "            pass\n",
    "                    \n",
    "      \n",
    "    # print(dico)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e11f1ece",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba0d2f91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import re\n",
    "\n",
    "# liste_complete_comms=[]\n",
    "# liste_dates=[]\n",
    "# for x in df[\"Date de l'acte\"]:\n",
    "#     # print(x)\n",
    "# #    if re.match(x,r\".* \\\\d{4} Commentaires .+\"):\n",
    "#     if re.match(r'''Date de l\\'acte (\\d){4} (.+)''',x):\n",
    "#      liste=x.split()\n",
    "     \n",
    "#      for pos in range(0,len(liste)):\n",
    "#         print (pos)\n",
    "#         print (liste[pos])\n",
    "#         if re.match(r'(\\d){4}',liste[pos]):\n",
    "#            deuxieme_moitie= []\n",
    "#         #    print(liste)\n",
    "#         #    print(pos)\n",
    "#         #    print(liste[pos])\n",
    "#            premiere_moitie =[]\n",
    "#            for z in range(pos+1, len(liste)):\n",
    "#                 # print (liste)\n",
    "#                 print (z)\n",
    "#                 print(liste[z])\n",
    "#                 deuxieme_moitie.append(liste[z])\n",
    "#                 # liste.pop(int(z))\n",
    "#                 str2= \" \".join(deuxieme_moitie)\n",
    "                \n",
    "#            for z in range (0,pos+1):\n",
    "#                 premiere_moitie.append(liste[z])\n",
    "#                 str1=\" \".join(premiere_moitie)\n",
    "                \n",
    "#            liste_complete_comms.append(str2)\n",
    "#            liste_dates.append(str1)\n",
    "#         else:\n",
    "#            pass\n",
    "#     else:\n",
    "#       liste_dates.append(x)\n",
    "#       liste_complete_comms.append('Pas de commentaires')\n",
    "# print(liste_complete_comms)\n",
    "# print(liste_dates)              \n",
    "# df.insert(loc=10, column=\"Commentaires\", value=liste_complete_comms)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a004cdc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "for x in range(1,34):\n",
    "  with open(f\"C:\\\\Users\\\\fanti\\\\Documents\\\\stage_highvision\\\\stage_higvision(1)\\\\stage_higvision(2)\\\\stage_higvision\\\\scrapping\\\\album{x}\\\\notices_lien_def{x}.txt\",'r',encoding=\"utf8\") as fichier:\n",
    "      contenu = fichier.read()\n",
    "      liens_notices_finaux = str(contenu).split()\n",
    "      # print(liens_notices_finaux)\n",
    "      liens_notices_uniques=[]\n",
    "      for i in liens_notices_finaux:\n",
    "         if i in liens_notices_uniques:\n",
    "            pass\n",
    "         else:\n",
    "            liens_notices_uniques.append(i)\n",
    "      compteur=0\n",
    "      datafull=[]\n",
    "      for z in liens_notices_uniques:\n",
    "            # print(z)\n",
    "            data=[]\n",
    "            response = requests.get(z)\n",
    "            response.encoding = 'ISO-8859-1'  # Très important pour les accents\n",
    "            reponse=response.content\n",
    "            print(response)\n",
    "            soup = bs(reponse, 'html.parser')\n",
    "            image = soup.find('img', title='image' ) \n",
    "            print(image)\n",
    "            if image:\n",
    "                lien = image.get(\"src\")\n",
    "                lien_complet= \"http://www2.culture.gouv.fr\"+lien\n",
    "                print(lien_complet)\n",
    "                data.append(lien_complet)\n",
    "            datafull.append(data)\n",
    "            print(datafull)\n",
    "            \n",
    "  with open(f\"C:\\\\Users\\\\fanti\\\\Documents\\\\stage_highvision\\\\stage_higvision(1)\\\\stage_higvision(2)\\\\stage_higvision\\\\scrapping\\\\album{x}\\\\image_lien_def{x}.txt\",'a',encoding=\"utf8\") as file_img:\n",
    "            for i in datafull:\n",
    "               \n",
    "               for k in i:\n",
    "                  \n",
    "                  file_img.write(k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2dc7901",
   "metadata": {},
   "outputs": [],
   "source": [
    "for x in range(1,34):\n",
    "    with open(f\"C:\\\\Users\\\\fanti\\\\Documents\\\\stage_highvision\\\\stage_higvision(1)\\\\stage_higvision(2)\\\\stage_higvision\\\\scrapping\\\\album{x}\\\\image_lien_def{x}.txt\",'r',encoding=\"utf8\") as liste_liens_img:\n",
    "        popopop = liste_liens_img.read()\n",
    "        liste_liens_beta= popopop.split(sep=\"v.jpg\")\n",
    "        liste_liens_defs=[]\n",
    "        infos_zones=[]\n",
    "        for i in liste_liens_beta:\n",
    "            if not i in liste_liens_defs:\n",
    "            #  print(i)\n",
    "             liste_liens_defs.append(str(i+\"2.htm\"))\n",
    "        for lien in liste_liens_defs:\n",
    "            # print(lien)\n",
    "            try:\n",
    "                reponse = requests.get(lien)\n",
    "                contenu= reponse.content\n",
    "                soupe=bs(contenu,'html.parser')\n",
    "                carte=soupe.find_all(\"area\")\n",
    "                infos_zones.append(carte)\n",
    "            except:\n",
    "               print(lien)\n",
    "               pass\n",
    "        with open(f\"C:\\\\Users\\\\fanti\\\\Documents\\\\stage_highvision\\\\stage_higvision(1)\\\\stage_higvision(2)\\\\stage_higvision\\\\scrapping\\\\album{x}\\\\images_zones{x}.txt\",'a',encoding=\"utf8\") as zones:\n",
    "           compteur=0\n",
    "           for l in infos_zones:\n",
    "              compteur+=1\n",
    "              zones.write(str(l))\n",
    "              zones.write(f\" Image numéro {compteur} \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7ebce15",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import os\n",
    "for x in range(1,34):\n",
    "    chemin_img = f\"C:\\\\Users\\\\fanti\\\\Documents\\\\stage_highvision\\\\stage_higvision(1)\\\\stage_higvision(2)\\\\stage_higvision\\\\scrapping\\\\album{x}\\\\\"\n",
    "    # os.makedirs(f\"{chemin_img}imagehd{x}\")\n",
    "    with open(f\"C:\\\\Users\\\\fanti\\\\Documents\\\\stage_highvision\\\\stage_higvision(1)\\\\stage_higvision(2)\\\\stage_higvision\\\\scrapping\\\\album{x}\\\\image_lien_def{x}.txt\",'r',encoding=\"utf8\") as liste_liens_img:\n",
    "        popopop = liste_liens_img.read()\n",
    "        liste_liens_beta= popopop.split(sep=\"v.jpg\")\n",
    "        liste_liens_defs=[]\n",
    "        infos_zones=[]\n",
    "        for i in liste_liens_beta:\n",
    "            if not i in liste_liens_defs:\n",
    "            #  print(i)\n",
    "             liste_liens_defs.append(str(i+\"2.jpg\"))\n",
    "        compteur=0\n",
    "        for lien in liste_liens_defs:\n",
    "           try:\n",
    "            print(lien)\n",
    "            compteur+=1\n",
    "            nom=f\"image{compteur}.jpg\"\n",
    "            chemin2=chemin_img+f\"imagehd{x}\\\\\"+nom\n",
    "            response= requests.get(lien)\n",
    "            print(response)\n",
    "            # print(lien)\n",
    "            with open (chemin2, \"wb\") as image:\n",
    "              image.write(response.content)\n",
    "           except:\n",
    "              print(f\"{lien} n'a pas fonctionné\")\n",
    "              pass\n",
    "           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d38dae86",
   "metadata": {},
   "outputs": [],
   "source": [
    "for x in range(1,34):\n",
    "    chemin = f\"C:\\\\Users\\\\fanti\\\\Documents\\\\stage_highvision\\\\stage_higvision(1)\\\\stage_higvision(2)\\\\resultats_scrapping\"\n",
    "    os.makedirs(f\"{chemin}\\\\annee{x}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dcb7f1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "for x in range (1, 34):\n",
    "    arrivee=f\"{chemin}\\\\annee{x}\"\n",
    "    chemin_img = f\"C:\\\\Users\\\\fanti\\\\Documents\\\\stage_highvision\\\\stage_higvision(1)\\\\stage_higvision(2)\\\\stage_higvision\\\\scrapping\\\\album{x}\\\\\"\n",
    "    shutil.copytree(chemin_img+f\"imagehd{x}\",arrivee)\n",
    "    shutil.copy(chemin_img+f\"images_zones{x}.txt\",arrivee)\n",
    "    chemin_csv=f\"C:\\\\Users\\\\fanti\\\\Documents\\\\stage_highvision\\\\stage_higvision(1)\\\\stage_higvision(2)\\\\stage_higvision\\\\scrapping_def\\\\scrap_final{x}\\\\final_v3_{x}.csv\"\n",
    "    shutil.copy(chemin_csv, arrivee)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd7b9665",
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "for x in range (1,2):\n",
    "    arrivee=f\"{chemin}\\\\annee{x}\"\n",
    "    chemin_img = f\"C:\\\\Users\\\\fanti\\\\Documents\\\\stage_highvision\\\\stage_higvision(1)\\\\stage_higvision(2)\\\\stage_higvision\\\\scrapping\\\\album{x}\\\\\"\n",
    "    shutil.copy(chemin_img+f\"images_zones{x}.txt\",arrivee)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26aaf562",
   "metadata": {},
   "outputs": [],
   "source": [
    "for x in range(1,34):\n",
    "    with open (f\"C:\\\\Users\\\\fanti\\\\Documents\\\\stage_highvision\\\\stage_higvision(1)\\\\stage_higvision(2)\\\\resultats_scrapping\\\\annee{x}\\\\images_zones{x}.txt\",\"r+\") as fichier:\n",
    "        print(fichier)\n",
    "        donnees = fichier.read()\n",
    "        print(donnees)\n",
    "        donnees=donnees.replace('href=\"javascript:MisNot',\" \")\n",
    "        donnees=donnees.replace('area alt=',\" \")\n",
    "        print(donnees)\n",
    "        \n",
    "    with open(f\"C:\\\\Users\\\\fanti\\\\Documents\\\\stage_highvision\\\\stage_higvision(1)\\\\stage_higvision(2)\\\\resultats_scrapping\\\\annee{x}\\\\images_zones_V2_{x}.txt\",\"w\") as fichier2:\n",
    "        fichier2.write(donnees)\n",
    "    os.remove(f\"C:\\\\Users\\\\fanti\\\\Documents\\\\stage_highvision\\\\stage_higvision(1)\\\\stage_higvision(2)\\\\resultats_scrapping\\\\annee{x}\\\\images_zones{x}.txt\")    \n",
    "        # fichier.write(donnees)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "scrapping",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
