{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65be983d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup as bs\n",
    "import requests\n",
    "import os\n",
    "import shutil\n",
    "# import tqdm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7866426",
   "metadata": {},
   "outputs": [],
   "source": [
    "url_0 = \"http://www2.culture.gouv.fr/documentation/arcade/salons.htm\"\n",
    "reponse_0 = requests.get(url_0)\n",
    "reponse_0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b14b1e7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "if str(reponse_0) == \"<Response [200]>\":\n",
    "    contenu_0= reponse_0.content\n",
    "    soup_0 = bs(contenu_0, \"html.parser\")\n",
    "    liste_0=soup_0.ul\n",
    "    liens=[]\n",
    "    for a in liste_0.find_all('a'):\n",
    "        lien=a.get(\"href\")\n",
    "        liens.append(lien)\n",
    "    print (liens)\n",
    "else:\n",
    "    print(f\"{url_0} ne réponds pas\")\n",
    "        \n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bf230b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_dossier= 0\n",
    "constante = \"http://www2.culture.gouv.fr\"\n",
    "liens_notices=[]\n",
    "for i in liens:\n",
    "    num_dossier+=1\n",
    "    url=str(i)\n",
    "    reponse_1=requests.get(url)\n",
    "    os.makedirs(f\"album{num_dossier}\",exist_ok=True)\n",
    "    chemin = f\"album{num_dossier}\"\n",
    "    contenu_1=reponse_1.content\n",
    "    if str(reponse_1) == \"<Response [200]>\":\n",
    "        if os.path.exists(chemin):\n",
    "            pass\n",
    "        else:\n",
    "            os.makedirs(chemin)\n",
    "        soup=bs(contenu_1, \"html.parser\")\n",
    "        \n",
    "        liens_notices=[]\n",
    "        for a in soup.find_all('a'):\n",
    "            lien = a.get(\"href\")\n",
    "            liens_notices.append(lien)\n",
    "        if len(liens_notices)>= 205:\n",
    "            lien_page_2 = liens_notices[-1]\n",
    "            requete_page_2= requests.get(constante+lien_page_2)\n",
    "            contenu_page2= requete_page_2.content\n",
    "            soup2 = bs(contenu_page2, \"html.parser\")\n",
    "            for a in soup2.find_all('a'):\n",
    "                lien = a.get(\"href\")\n",
    "                liens_notices.append(lien)\n",
    "            with open(f\"album{num_dossier}\\\\fichier{num_dossier}.txt\",'w')as fichier:\n",
    "                compteur = 0\n",
    "                for i in liens_notices:\n",
    "                 compteur +=1\n",
    "                 fichier.write(f\" {i}\")\n",
    "\n",
    "        else:\n",
    "            with open(f\"album{num_dossier}\\\\fichier{num_dossier}.txt\",'w') as fichier:\n",
    "                compteur = 0\n",
    "                for i in liens_notices:\n",
    "                 compteur +=1\n",
    "                 fichier.write(f\" {i}\")\n",
    "\n",
    "\n",
    "    else:\n",
    "        print (f\"{url} ne réponds pas\")\n",
    "        pass\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b79f6d70",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "# print(liens_notices)\n",
    "# len(liens_notices)\n",
    "for x in range (1,34):\n",
    "   liens_notices_propres = []\n",
    "   with open(f\"album{x}\\\\fichier{x}.txt\",'r',encoding=\"utf8\") as fichier:\n",
    "     \n",
    "      contenu = fichier.read()\n",
    "      print(contenu)\n",
    "      \n",
    "      liens_notices= str(contenu).split()\n",
    "      print(liens_notices)\n",
    "      for i in liens_notices:\n",
    "        if re.match(r\".*ACTION=RETROUVER&FIELD.*\",i):\n",
    "            liens_notices_propres.append(i)\n",
    "            print (liens_notices_propres)\n",
    "            with open(f\"album{x}\\\\notices_propres{x}.txt\",'w') as fichier:\n",
    "               compteur=0\n",
    "               for i in liens_notices_propres:\n",
    "                  compteur+=1\n",
    "                  fichier.write(f\" {i}\")\n",
    "        else:\n",
    "           pass\n",
    "          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1037208",
   "metadata": {},
   "outputs": [],
   "source": [
    "for x in range (1,34):\n",
    "   liens_notices_propres = []\n",
    "   with open(f\"album{x}\\\\notices_propres{x}.txt\",'r',encoding=\"utf8\") as fichier:\n",
    "     \n",
    "      contenu = fichier.read()\n",
    "      \n",
    "      liens_notices_propres = str(contenu).split()\n",
    "      # print(liens_notices)\n",
    "   liens_def = []\n",
    "   for i in liens_notices_propres:\n",
    "      i2 = constante+str(i)\n",
    "      liens_def.append(i2)\n",
    "      with open(f\"album{x}\\\\notices_lien_def{x}.txt\",'w') as fichier:\n",
    "               # compteur=0\n",
    "               for y in liens_def:\n",
    "                  # compteur+=1\n",
    "                  fichier.write(f\" {y}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0f22daf",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(\"scrapping_def\",exist_ok=True)\n",
    "for x in range(1,34):\n",
    "  with open(f\"album{x}\\\\notices_lien_def{x}.txt\",'r',encoding=\"utf8\") as fichier:\n",
    "      contenu = fichier.read()\n",
    "      liens_notices_finaux = str(contenu).split()\n",
    "      # print(liens_notices_finaux)\n",
    "      liens_notices_uniques=[]\n",
    "      for i in liens_notices_finaux:\n",
    "         if i in liens_notices_uniques:\n",
    "            pass\n",
    "         else:\n",
    "            liens_notices_uniques.append(i)\n",
    "      compteur=0\n",
    "      datafull=[]\n",
    "      for z in liens_notices_uniques:\n",
    "            # print(z)\n",
    "            data=[]\n",
    "            response = requests.get(z)\n",
    "            response.encoding = 'ISO-8859-1'  # Très important pour les accents\n",
    "            reponse=response.content\n",
    "            soup = bs(reponse, 'html.parser')\n",
    "            table = soup.find('table', valign='TOP' )  \n",
    "            if table:\n",
    "                 with open(f\"scrapping_def\\\\resultat{x}_et_{compteur}.txt\",'w',encoding=\"utf8\") as result:\n",
    "                  compteur+=1\n",
    "                  rows = table.find_all('tr')\n",
    "                  # print(rows)\n",
    "                  for row in rows:\n",
    "                     result.write(row.get_text(strip=True, separator=';'))\n",
    "                  #   data.append(row.get_text(strip=True, separator=';')) \n",
    "                  #   result.write(str(data))\n",
    "                  #   datafull.append(data)    \n",
    "            with open(f\"scrapping_def\\\\resultatfull_{x}.txt\",'w',encoding=\"utf8\") as result_full:\n",
    "             result_full.write(row.get_text(strip=True, separator=';'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d3ae440",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import os\n",
    "balises = [\"Cote\",\"Auteur\",\"Dénomination\",\"Technique\",\"Matériau\",\"Titre\",\"Thème représenté\",\"Mot-clé\",\"Stade de la création\",\"Exposition\",\"Nature de l'acte\",\"Procédure\",\"Date de l'acte\",\"Provenance\",\"Commentaires\",\"Localisation\",\"Édifice\",\"Nom de l'édifice\",\"voir aussi\",\"Copyright\",\" N° notice\"]\n",
    "\n",
    "for i in range(1,34):\n",
    "    grosdico=[]\n",
    "    # os.makedirs(f\"C:\\\\Users\\\\Fantin.LEBER\\\\Documents\\\\stage_higvision\\\\scrapping_def\\\\scrap_final{i}\")\n",
    "    for z in range (1,200):\n",
    "        try:\n",
    "        \n",
    "            dico={}\n",
    "            with open(f\"scrapping_def\\\\resultat{i}_et_{z}.txt\",'r',encoding=\"utf8\") as fichier:\n",
    "                infos=fichier.read()\n",
    "                # print (infos)\n",
    "                # os.makedirs(f\"C:\\\\Users\\\\Fantin.LEBER\\\\Documents\\\\stage_higvision\\\\scrapping_def\\\\scrap_final{i}\")\n",
    "                data= infos.split(\";\")\n",
    "                \n",
    "                # print(data)\n",
    "                # print(data)\n",
    "                balises_y = []\n",
    "                for y in range(0,(len(data))):\n",
    "                    # print(len(data))\n",
    "                    \n",
    "                    # print(data[y])\n",
    "                    for avion in balises:\n",
    "                        # print(avion)\n",
    "                        if str(data[y])== str(avion):\n",
    "                            balises_y.append(y)\n",
    "                        # else:\n",
    "                        #     pass\n",
    "                    # print(balises_y)\n",
    "                    \n",
    "                for w in range(0,(len(balises_y))):\n",
    "                    # print(len(balises_y))        \n",
    "                    try:\n",
    "                        infos=[]\n",
    "                        for position in range ((balises_y[w]),(balises_y[w+1])):\n",
    "                                infos.append(data[position])\n",
    "                       \n",
    "                        for jpp in range (1, len(infos)):\n",
    "                            for youpi in balises:\n",
    "                                if infos[jpp]==youpi:\n",
    "                                    infos.pop(jpp)\n",
    "                    except:\n",
    "                      pass\n",
    "                    dico[str(data[int(balises_y[int(w)])])]= \" \".join(str(element) for element in infos)\n",
    "                # print(dico)\n",
    "                grosdico.append(dico)\n",
    "                # print(grosdico)\n",
    "                os.makedirs(f\"scrapping_def\\\\scrap_final{i}\", exist_ok=True)          \n",
    "                        # print(dico)\n",
    "                with open (f\"scrapping_def\\\\scrap_final{i}\\\\final_v3_{i}.csv\",\"w\",newline=\"\") as final:\n",
    "                            writer = csv.DictWriter(final, fieldnames=balises)\n",
    "                            writer.writeheader()\n",
    "                            writer.writerows(grosdico)\n",
    "                            print(f\"final_v3_{i} créé\") \n",
    "        except:\n",
    "            pass\n",
    "                    \n",
    "      \n",
    "    # print(dico)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a004cdc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "for x in range(1,34):\n",
    "  with open(f\"album{x}\\\\notices_lien_def{x}.txt\",'r',encoding=\"utf8\") as fichier:\n",
    "      contenu = fichier.read()\n",
    "      liens_notices_finaux = str(contenu).split()\n",
    "      # print(liens_notices_finaux)\n",
    "      liens_notices_uniques=[]\n",
    "      for i in liens_notices_finaux:\n",
    "         if i in liens_notices_uniques:\n",
    "            pass\n",
    "         else:\n",
    "            liens_notices_uniques.append(i)\n",
    "      compteur=0\n",
    "      datafull=[]\n",
    "      for z in liens_notices_uniques:\n",
    "            # print(z)\n",
    "            data=[]\n",
    "            response = requests.get(z)\n",
    "            response.encoding = 'ISO-8859-1'  # Très important pour les accents\n",
    "            reponse=response.content\n",
    "            print(response)\n",
    "            soup = bs(reponse, 'html.parser')\n",
    "            image = soup.find('img', title='image' ) \n",
    "            print(image)\n",
    "            if image:\n",
    "                lien = image.get(\"src\")\n",
    "                lien_complet= \"http://www2.culture.gouv.fr\"+lien\n",
    "                print(lien_complet)\n",
    "                data.append(lien_complet)\n",
    "            datafull.append(data)\n",
    "            print(datafull)\n",
    "            \n",
    "  with open(f\"album{x}\\\\image_lien_def{x}.txt\",'a',encoding=\"utf8\") as file_img:\n",
    "            for i in datafull:\n",
    "               \n",
    "               for k in i:\n",
    "                  \n",
    "                  file_img.write(k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2dc7901",
   "metadata": {},
   "outputs": [],
   "source": [
    "for x in range(1,34):\n",
    "    with open(f\"album{x}\\\\image_lien_def{x}.txt\",'r',encoding=\"utf8\") as liste_liens_img:\n",
    "        popopop = liste_liens_img.read()\n",
    "        liste_liens_beta= popopop.split(sep=\"v.jpg\")\n",
    "        liste_liens_defs=[]\n",
    "        infos_zones=[]\n",
    "        for i in liste_liens_beta:\n",
    "            if not i in liste_liens_defs:\n",
    "            #  print(i)\n",
    "             liste_liens_defs.append(str(i+\"2.htm\"))\n",
    "        for lien in liste_liens_defs:\n",
    "            # print(lien)\n",
    "            try:\n",
    "                reponse = requests.get(lien)\n",
    "                contenu= reponse.content\n",
    "                soupe=bs(contenu,'html.parser')\n",
    "                carte=soupe.find_all(\"area\")\n",
    "                infos_zones.append(carte)\n",
    "            except:\n",
    "               print(lien)\n",
    "               pass\n",
    "        with open(f\"album{x}\\\\images_zones{x}.txt\",'a',encoding=\"utf8\") as zones:\n",
    "           compteur=0\n",
    "           for l in infos_zones:\n",
    "              compteur+=1\n",
    "              zones.write(str(l))\n",
    "              zones.write(f\" Image numéro {compteur} \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7ebce15",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for x in range(1,34):\n",
    "    chemin_img = f\"album{x}\\\\\"\n",
    "    os.makedirs(f\"{chemin_img}imagehd{x}\")\n",
    "    with open(f\"album{x}\\\\image_lien_def{x}.txt\",'r',encoding=\"utf8\") as liste_liens_img:\n",
    "        popopop = liste_liens_img.read()\n",
    "        liste_liens_beta= popopop.split(sep=\"v.jpg\")\n",
    "        liste_liens_defs=[]\n",
    "        infos_zones=[]\n",
    "        for i in liste_liens_beta:\n",
    "            if not i in liste_liens_defs:\n",
    "            #  print(i)\n",
    "             liste_liens_defs.append(str(i+\"2.jpg\"))\n",
    "        compteur=0\n",
    "        for lien in liste_liens_defs:\n",
    "           try:\n",
    "            print(lien)\n",
    "            compteur+=1\n",
    "            nom=f\"image{compteur}.jpg\"\n",
    "            chemin2=chemin_img+f\"imagehd{x}\\\\\"+nom\n",
    "            response= requests.get(lien)\n",
    "            print(response)\n",
    "            # print(lien)\n",
    "            with open (chemin2, \"wb\") as image:\n",
    "              image.write(response.content)\n",
    "           except:\n",
    "              print(f\"{lien} n'a pas fonctionné\")\n",
    "              pass\n",
    "           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d38dae86",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(\"resultats_scrapping\", exist_ok=True)\n",
    "for x in range(1,34):\n",
    "    chemin = f\"resultats_scrapping\"\n",
    "    os.makedirs(f\"{chemin}\\\\annee{x}\", exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd7b9665",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for x in range (1,34):\n",
    "    arrivee=f\"{chemin}\\\\annee{x}\"\n",
    "    chemin_img = f\"album{x}\\\\\"\n",
    "    shutil.copy(chemin_img+f\"images_zones{x}.txt\",arrivee)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26aaf562",
   "metadata": {},
   "outputs": [],
   "source": [
    "for x in range(1,34):\n",
    "    with open (f\"resultats_scrapping\\\\annee{x}\\\\images_zones{x}.txt\",\"r+\") as fichier:\n",
    "        print(fichier)\n",
    "        donnees = fichier.read()\n",
    "        print(donnees)\n",
    "        donnees=donnees.replace('href=\"javascript:MisNot',\" \")\n",
    "        donnees=donnees.replace('area alt=',\" \")\n",
    "        print(donnees)\n",
    "        \n",
    "    with open(f\"resultats_scrapping\\\\annee{x}\\\\images_zones_V2_{x}.txt\",\"w\") as fichier2:\n",
    "        fichier2.write(donnees)\n",
    "    os.remove(f\"resultats_scrapping\\\\annee{x}\\\\images_zones{x}.txt\")    \n",
    "        # fichier.write(donnees)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "scrapping",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
